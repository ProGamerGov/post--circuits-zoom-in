<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="theme-color" content="#000000" />

    <title>Zoom In: Features and Circuits</title>
  </head>

  <body>
    <d-front-matter>
      <code style="display: none;" type="text/json"
        >{ "title": "Zoom In: Features and Circuits", "description": "", "published": false,
        "password": "circuits", "authors": [{ "author": "TBD", "authorURL": "",
        "affiliations": [{ "name": "", "url": "" }] }] }</code
      >
    </d-front-matter>
    <d-title></d-title>
    <d-byline></d-byline>
    <d-article>
      <d-contents>
        <nav class="l-text toc figcaption">
          <h3>Contents</h3>
          <div><a href="#introduction">Introduction</a></div>
          <div><a href="#three-speculative-claims">Three Speculative Claims</a></div>
          <div><a href="#claim-1">Claim 1: Features</a></div>
          <ul>
            <li><a href="#claim-1-curves">Example 1: Curve Detectors</a></li>
            <li><a href="#claim-1-hilo">Example 2: High-Low Frequency Detectors</a></li>
            <li><a href="#claim-1-dog">Example 3: Pose-Invariant Dog Head Detector</a></li>
            <li><a href="#claim-1-polysemantic">Polysemantic Neurons</a></li>
          </ul>
          <div><a href="#claim-2">Claim 2: Circuits</a></div>
          <ul>
            <li><a href="#claim-2-curves">Circuit 1: Curve Detectors</a></li>
            <li><a href="#claim-2-dog">Circuit 2: Oriented Dog Head Detection</a></li>
            <li><a href="#claim-2-superposition">Circuit 3: Cars in Superposition</a></li>
          </ul>
          <div><a href="#claim-3">Claim 3: Universality</a></div>
          <div><a href="#natural-science">Interpretability as a Natural Science</a></div>
          <div><a href="#closing">Closing Thoughts</a></div>
        </nav>
      </d-contents>

      <div>
      <h2 id="introduction">Introduction</h2>
      <p>
      Many important transition points in the history of science have been moments when science “zoomed in.”
      At these points, we develop some visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.
      </p>
      <p>
      For example, microscopes let us see cells, leading to cellular biology. Science zoomed in. Several techniques including x-ray crystallography let us see DNA, leading to the molecular revolution. Science zoomed in. Atomic theory. Subatomic particles. Neuroscience. Science zoomed in.
      </p>
      <p>
      These transitions weren’t just a change in precision: they were qualitative changes in what the objects of scientific inquiry are.
      For example, cellular biology isn’t just more careful zoology. It's a new kind of inquiry that dramatically shifts what we can understand.
      </p></div>

      <figure class="l-body-outset">
        <img src="./images/micrographia2.png" />
        <figcaption>
          Hooke’s Micrographia revealed a rich microscopic world as seen
          through a microscope, including the initial discovery of cells.
          <br />Images from the National Library of Wales.
        </figcaption>
      </figure>

      <p>Just as the microscope revealed a tantalizing microscopic world, visualizations of artificial neural networks have revealed tantalizing hints and glimpses of a rich inner world within our models. This has led us to wonder: Is it possible that deep learning is at a similar transition point? Could there be a kind of cellular biology or neuroscience analogue of deep learning? And if so, what might such a field look like? What are its objects of study, and what would rigorous inquiry into them look like?</p>

      <p>Our tentative answer is that you get the study of features and the neural circuits that implement them.</p>

      <p>Exploring these objects over the last year and a half has led to the circuits project, an open scientific collaboration hosted on the Distill slack. In future articles, the collaboration will publish detailed explorations of this inner world. But before we do that, we wanted to offer a high-level overview of our thinking and some of the working principles that we’ve found useful in this line of research.</p>

      <figure class="l-body-outset" style="margin-bottom: 0px;">
        <img src="./images/deepdream.png" />
        <figcaption>
          Over the last few years, we've seen many incredible visualizations<d-cite bibtex-key='karpathy2015visualizing,erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug,zeiler2014visualizing,fong2017interpretable,kindermans2017patternnet,reif2019visualizing,carter2019activation'></d-cite> and analyses<d-cite bibtex-key='mikolov2013distributed,radford2017learning,zhou2014object,netdissect2017'></d-cite> hinting at a rich world of internal features in modern
          neural networks. Above, we see a <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">DeepDream</a><d-cite bibtex-key="mordvintsev2015inceptionism"></d-cite> image, which sparked a great deal of excitement in this space.
        </figcaption>
      </figure>

      <hr>

      <h2 id="three-speculative-claims">Three Speculative Claims</h2>

      <p>One of the earliest articulations of something approaching modern cell theory were three claims by Theodor Schwann — who you may know for Schwann cells — in 1839:</p>


      <figure class="l-body-outset claim-figure">
        <div>
          <img src="./images/schwann-book.png" />
          <div style="">
            <h4>Schwann’s Claims about Cells</h4>

            <div class="claim">
              <div class="claim-header">Claim 1</div>
              The cell is the unit of structure, physiology, and organization in living things.
            </div>

            <div class="claim">
              <div class="claim-header">Claim 2</div>
              The cell retains a dual existence as a distinct entity and a building block in the construction of organisms.
            </div>

            <div class="claim">
              <div class="claim-header">Claim 3</div>
              Cells form by free-cell formation, similar to the formation of crystals.
            </div>

            <div class="figcaption" style="margin-top: 20px;">
              This translation/summarization of Schwann's claims can be found in many biology texts; we were unable to determine what the original source of the translation is. The image of Schwann's book <i>Mikroskopische Untersuchungen über die Uebereinstimmung in der Struktur und dem Wachsthum der Thiere und Pflanzen</i> from the <a href="http://www.deutschestextarchiv.de/book/show/schwann_mikroskopische_1839">Deutsches Textarchiv</a>.
            </div>

          </div>
        </div>
      </figure>

      <p>The first two of these claims are likely familiar, persisting in modern cellular theory. The third is likely not familiar, since it turned out to be horribly wrong.</p>

      <p>We believe there’s a lot of value in articulating a strong version of something you believe may be true, even it might be false like Schwann’s third claim. In this spirit, we offer three claims about neural networks. They are intended both as empirical claims about the nature of neural networks, and also as normative claims about how it’s useful to understand neural networks.</p>


      <figure class="l-body-outset claim-figure">
        <div>
          <img src="./images/atlas-book-crop.png" style="width: 220px;"/>
          <div>
            <h4 style="margin-top: 0px;">Three Speculative Claims about Neural Networks</h4>

            <div class="claim">
              <div class="claim-header">Claim 1: Features</div>
              Features are the fundamental unit of neural networks.<br>
              These features correspond to directions.<d-footnote>
                By "direction" we mean a linear combination of neurons in a layer.
                You can think of this as a direction vector in the vector space of activations of neurons in a given layer.
                Often, we find it most helpful to talk about individual neurons,
                but we'll see that there are some cases where other combinations are a more useful way to analyze networks
                -- especially when neurons are "polysemantic."
                (See the <a href="glossary">glossary</a> for a detailed definition.)
              </d-footnote>
              They can be rigorously studied and understood.
            </div>

            <div class="claim">
              <div class="claim-header">Claim 2: Circuits</div>
              Features are connected by weights, forming circuits.<d-footnote>
                A "circuit" is a computational subgraph of a neural network.
                It consists of a set of features, and the weighted edges that go between them in the original
                network.
                Often, we study quite small circuits -- say with less than a dozen features -- but they can also be much larger.
                (See the <a href="glossary">glossary</a> for a detailed definition.)
              </d-footnote><br>
              These circuits can also be rigorously studied and understood.
            </div>

            <div class="claim">
              <div class="claim-header">Claim 3: Universality</div>
              Analogous features and circuits form across models and tasks.
            </div>

            <div class="figcaption big-only" style="margin-top: 20px;">
              Left: An <a href="https://distill.pub/2019/activation-atlas/">activation atlas</a><d-cite bibtex-key="carter2019activation"></d-cite> visualizing the space neural network features can represent.
            </div>

          </div>
        </div>
      </figure>



      <p>These claims are deliberately speculative. But we believe that, if true, they could be a basis for a “zoomed in” field. And we think that there’s enough evidence pointing in the direction of all three of these claims for them to be worth serious consideration and investigation.</p>

      <p>In the following sections, we’ll discuss each of these claims individually and present some of the evidence that has led us to believe they might be true.</p>




      <hr>

      <h2 id="claim-1">Claim 1: Features</h2>

      <style>
      .claim-quote {
        border-left: 1px solid #CCC;
        padding-left:  20px;
        color: #555;
        margin-bottom: 30px;
      }
      </style>

      <p class="claim-quote">
        Features are the fundamental unit of neural networks.
        They correspond to directions. They can be rigorously studied and understood.
      </p>

      <p>
        We believe that neural networks consist of meaningful, understandable features. Early layers consist of features like edge or curve detectors, while later layers have features like floppy ear detectors or wheel detectors.
        The community is divided on whether this is true.
        While many researchers treat the existence of meaningful neurons as an almost trivial fact and there's a small literature studying them, many others are deeply skeptical and believe that past cases of neurons that seemed to track meaningful latent variables are mistaken.
      <d-footnote>The community disagreement on meaningful features is hard to pin down, and only partially expressed in the literature. Foundational descriptions of deep learning often describe neural networks as detecting a hierarchy of meaningful features <d-cite bibtex-key='lecun2015deep'></d-cite>, and a number of papers have been written demonstrating seemingly meaningful features in different domains domains<d-cite bibtex-key='mikolov2013distributed,karpathy2015visualizing,radford2017learning,zhou2014object,olah2017feature,netdissect2017'></d-cite>. At the same time, a more skeptical parallel literature has developed suggesting that neural networks primarily or only focus on texture, local structure, or imperceptible patterns <d-cite bibtex-key='jo2017measuring,geirhos2018imagenet,brendel2019approximating,ilyas2019adversarial'></d-cite>, that meaningful features, when they exist, are less important than uniterptetable ones <d-cite bibtex-key='morcos2018importance'> and that seemingly interpretable neurons may be misunderstood <d-cite bibtex-key='donnelly2019interpretability'></d-cite>. Although many of these papers express a highly nuanced view, that isn’t always how they’ve been understood. A number of media articles have been written embracing strong versions of these views, and we anecdotally find that the belief that neural networks don’t understand anything more than texture is quite common. Finally, people often have trouble articulating their exact views, because they don’t have clear language for articulating nuances between “a texture detector highly correlated with an object” and “an object detector.”</d-footnote>
      Nevertheless, thousands of hours of studying individual neurons have led us to believe the typical case is that neurons (or in some cases, other directions in the vector space of neuron activations) are understandable.

      <p>
        Of course, being understandable doesn't mean being simple or easily understandable.
        Many neurons are initially mysterious and don't follow our a priori guesses of what features will exist!
        However, our experience is that there's usually a simple explanation behind these neurons, and that they're actually doing something quite natural.
        For example, we were initially confused by high-low frequency detectors (discussed below) but in retrospect, they are simple and elegant.

<!--
      <p>
        Whether or not features are understandable isn't just a curiosity.
        Every approach to interpretability needs some way out of the curse of dimensionality.
        For example, saliency methods rely on locality -- on the network having roughly linear behavior in some neighborhood.
        For us, the way out of the curse of dimensionality is features being individually understandable.<d-footnote>

        </d-footnote>
        Going through a network neuron by neuron is still a massive undertaking, but it isn't an exponentially growing impossible one.
-->

      <p>
        This introductory essay will only give an overview of a couple examples we think are illustrative, but it will be followed both by deep dives carefully characterizing individual features, and broad overviews sketching out all the features we understand to exist.

      <p>
        Regardless of whether we're correct or mistaken about meaningful features,
        we believe this is an important question for the community to resolve.
        We hope that introducing several specific carefully explored examples of seemingly understandable features will help advance the dialogue.

<!--

      <p>Part of this claim is trivially true. Directions in activation space (the vector space spanned by neurons) include neurons. Neural networks can obviously be thought of as being composed of neurons, and so there’s an uninteresting sense in which they are at least a unit of neural networks. The interesting part of the claim is that they can, with sufficient effort, be rigorously studied and understood. And as a result, they are in some sense the “right” unit to think about neural networks in terms of.</p>

      <p>In light of the existing disagreement, we will try to hold ourselves to a high evidentiary standard in making such claims. This introductory essay will only give an overview of some of the examples we’ve found most compelling, but it will be followed by several deep dives characterizing individual features.

-->

      <h3 id="claim-1-curves">Example 1: Curve Detectors</h3>

      <p>
        Curve detecting neurons can be found in every non-trivial vision model we've carefully examined.
        These units are interesting because they straddle the boundary between features the community broadly agrees exist (eg. edge detectors) and features for which there's significant skepticism (more complex shapes).
      </p>

      <p>We'll focus on curve detectors in InceptionV1 layer mixed3b (an early layer). These units responded to curved lines and boundaries with a size on the order of 40 pixels. They are also slightly additionally excited by perpendicular lines along the boundary of the curve, and prefer the two sides of the curve to be different colors. Each unit in the family responds to curves in a different orientation, jointly spanning the full range of orientations.

      <p>
        It's important to distinguish curve detectors from other units which may seem superficially similar.
        In particular, there are many units which use curves to detect a curved sub-component (eg. circles, spirals, S-curves, hourglass shape, 3d curvature, ...).
        There are also units which respond to curve related shapes like lines or sharp corners.
        We do not consider these units to be curve detectors.

      <figure style="grid-column-start: text-start; grid-column-end: text-end;">
        <img src="./images/curves.png"/>
        <!--<figcaption></figcaption>-->
      </figure>

      <!--
      <p>
        It's worth noting that many neurons-->

      <p>
        Are the curve detectors really curve detectors?
        We’ve dedicated an entire article to exploring this in depth,
        but the summary is that we think the evidence is quite strong.
        We offer seven arguments.
        (The last three arguments are based on circuits, which we’ll discuss in the next section.)
      </p>
      <p>
        It's worth noting that none of these arguments is curve specific -- they're a useful, general toolkit for testing our understanding of other neurons as well.
      </p>


        <figure class="arguments-figure">
          <div>
            <img src="images/arg-fv.png"></img>
            <div>
              <h4>Argument 1: Feature Visualization</h4>
              <p>Optimizing the input to cause curve detectors to fire reliably produces curves. This establishes a causal link, since everything in the resulting image was added to cause the neuron to fire more. (You can learn more about feature visualization <a href="https://distill.pub/2017/feature-visualization/">here</a>.)</p>
            </div>
          </div>
          <div>
            <img src="images/arg-data.png"></img>
            <div>
              <h4>Argument 2: Dataset Examples</h4>
              <p>The ImageNet images that cause these neurons to strongly fire are reliably curves in the expected orientation. The images that cause them to fire moderately are generally less perfect curves or curves off orientation.</p>
            </div>
          </div>
          <div>
            <img src="images/arg-synthetic.png" style="filter: brightness(97%);"></img>
            <div>
              <h4>Argument 3: Synthetic Examples</h4>
              <p>Curve detectors respond as expected to a range of synthetic curves images created with varying orientations, curvatures, and backgrounds. They fire only near the expected orientation, and do not fire strongly for straight lines or sharp corners.</p>
            </div>
          </div>
          <div>
            <img src="images/arg-tune.png" style="filter: brightness(97%);"></img>
            <div>
              <h4>Argument 4: Joint Tuning</h4>
              <p>If we take dataset examples that cause a neuron to fire and rotate them, they gradually stop firing and the curve detectors in the next orientation begins firing. This shows that they detect rotated versions of the same thing. Together, they tile the full 360 degrees of potential orientations.</p>
            </div>
          </div>
          <div>
            <img src="images/arg-weights.png"></img>
            <div>
              <h4>Argument 5: Feature implementation
                <span style='color: #888; margin-left: 14px;'>(circuit-based argument)</span></h4>
              <p>By looking at the circuit constructing the curve detectors, we can literally read a curve detection algorithm off of the weights. We also don’t see anything suggestive of a second alternative cause of firing, although there are many smaller weights we don’t understand the role of.</p>
            </div>
          </div>
          <div>
            <img src="images/arg-use.png"></img>
            <div>
              <h4>Argument 6: Feature use
                <span style='color: #888; margin-left: 14px;'>(circuit-based argument)</span></h4>
              <p>The downstream clients of curve detectors are features that naturally involve curves (eg. circles, 3d curvature, spirals…). The curve detectors are used by these clients in the way our theory of the two features would predict.</p>
            </div>
          </div>
          <div>
            <img src="images/arg-hand.png"></img>
            <div>
              <h4>Argument 7: Handwritten Circuits
                <span style='color: #888; margin-left: 14px;'>(circuit-based argument)</span></h4>
              <p>Base on our understanding of how curve detectors are implemented,
                we can do a clean room reimplementation,
                taking a network with all zero weights and hand setting weights to reimplement curve detection.
                These weights are an understandable curve detection algorithm, and significantly mimic the original curve detectors.</p>
            </div>
          </div>
        </figure>

      <p>These experiments don’t rule out the possibility of some rare secondary case where curve detectors fire for a  different kind of stimulus. But they do seem to establish
        that curves cause these neurons to fire,
        that each unit responds to curves at different angular orientations,
        and that if there are other stimuli that cause them to fire those are rare or cause much weaker activations.

      <p>
        All of these arguments will be explored in detail in the forthcoming articles on curve detectors and curve detection circuits.

      <h3 id="claim-1-hilo">Example 2: High-Low Frequency Detectors</h3>

      <p>
        Curve detectors are the kind of feature one might guess a priori exist in neural networks.
        Give that they're present, it's not surprising we can understand them.
        But what about features that aren't intuitive? Can we also understand those?


      <p>
        High-low frequency detectors are an example of a less type of features that are, in retrospect, elegant and simple. We find them in early vision. They look for low-frequency patterns on one side of their receptive field, and high-frequency patterns on the other side. Like curve detectors, high-low frequency detectors are found in families of features that look for the  same thing in different orientations.

      <figure style="grid-column-start: text-start; grid-column-end: page-end;">
        <img src="./images/high-low.png" />
        <!--<figcaption></figcaption>-->
      </figure>


      <p>Why are high-low frequency detectors useful to the network? They seem to be one of several heuristics for detecting the boundaries of objects, especially when the background is out of focus. In future articles, we’ll explore how they’re used in the construction of sophisticated boundary detectors.

        <p>
          (One hope some researchers have for interpretability is that understanding models will be able to teach us better abstractions for thinking about the world <d-cite bibtex-key="carter2017using"></d-cite>. High-low frequency detectors are, perhaps, an example of a small success in this: a natural, useful visual feature that we didn't anticipate in advance.)

      <p>All six of the techniques we used to interrogate curve neurons can also be used to study high-low frequency neurons with some tweaking -- for instance, rendering synthetic high-low frequency examples. Again we believe these arguments collectively provide strong support for the idea that these really are a family of high-low frequency contrast detectors.

      <h3 id="claim-1-dog">Example 3: Pose-Invariant Dog Head Detector</h3>

      <p>So far, we’ve looked at low-level visual features. What about high-level features?

      <p>Let’s consider this unit which we believe to be a pose-invariant dog detector. As with any neuron, we can create a feature visualization and collect dataset examples. If you look at the feature visualization, the geometry is… not possible, but very informative about what it’s looking for! The dataset examples back this up!

      <figure style="grid-column-start: text-start; grid-column-end: text-end;">
        <img src="./images/dog-pose.png" style="max-width: 650px;"/>
        <!--<figcaption></figcaption>-->
      </figure>

      <p>The combination of feature visualization and dataset examples are quite a strong argument. Feature visualization gets at the underlying cause of a neuron firing, while dataset examples get at whether there’s other cases.

      <p>
        Again, we can bring all our other approaches to analyzing a neuron to bare.
        For example, we can use a 3D model to generate synthetic dog head images from different angles.
        But that can be come a lot of effort for these higher-level, more abstract features.
        Thankfully, our circuits-based arguments -- which we'll discuss more soon -- will continue to be easy to apply, and give us really powerful tools for understanding high-level features that don't require a lot of effort.


      <h3 id="claim-1-polysemantic">Polysemantic Neurons</h3>

      <p>This essay may be giving you an overly rosy picture: perhaps every neuron yields a nice, human-understandable concept if one seriously investigates it?

      <p>
        Alas, this is not the case.
        Neural networks often contain “polysemantic neurons” which respond to multiple unrelated inputs.
        For example, InceptionV1 contains one neuron that responds to cat faces, fronts of cars, and cat legs.

      <figure style="max-width: 600px; ">
        <img src="./images/polysemantic.png"/>
        <figcaption>4e:55 is a polysemantic neuron which responds to cat faces, fronts of cars, and cat legs. It was dicussed in more depth in <a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a> <d-cite key="olah2017feature"></d-cite>.</figcaption>
      </figure>

      <p>To be clear, this neuron isn’t responding to some commonality of cars and cat faces. Feature visualization shows us that it’s looking for the eyes and whiskers of a cat, for furry legs, and for shiny fronts of cars &mdash; not some subtle shared feature.

      <p>We can still study such features, characterizing each different case they fire. But it’s still a major source of difficulty in understanding neural networks. (Our hope is that there might someday be alternative basis for describing neural networks that avoids this.) Despite these challenges, we’ll see in the next section that it’s still possible to reason about circuits with polysemantic neurons, and in fact that circuits will hint at why polysemantic neurons form.





      <hr>

      <h2 id="claim-2">Claim 2: Circuits</h2>

      <p class="claim-quote">
        Features are connected by weights, forming circuits. These circuits can also be rigorously studied and understood.
      </div>

      <p>Part of this claim is trivially true. Of course features are connected by weights &mdash; that’s the definition of a neural network! &mdash; and of course you can consider a subgraph of how those features interact. In fact, these circuits are a really natural thing to turn your attention to once you accept Claim 1: if you can understand features, why wouldn’t you start looking at how they’re wired together?

      <p>The remarkable thing is how tractable and meaningful these circuits seem to be as objects of study. When we began looking, we expected to find something quite messy. Instead, we’ve found beautiful rich structures, often with symmetry to them. Once you understand what features they’re connecting together, the individual floating point number weights in your neural network become meaningful! <i>You can literally read meaningful algorithms off of the weights.</i>

      <p>Let's consider some examples.

      <h3 id="claim-2-curves">Circuit 1: Curve Detectors</h3>

      <p>In the previous section, we discussed curve detectors, a family of units detecting curves in different angular orientations. In this section, we’ll explore how curve detectors connect to the rest of the model.

      <p>Of course, there’s a long tail of small connections to different parts of early vision. But the primary story seems to be this: At the previous layer, the model has edge detectors and a less sophisticated set of curve detectors. These are combined to create our curve detectors. Then curve detectors are used in the next layer to create 3D geometry and complex shape detectors.

      <p>For this introduction, we’ll focus on the interaction of the early curve detectors and our full curve detectors.

        <figure class="l-page">
          <img src="./images/curve-circuit.png"/>
          <!--<figcaption></figcaption>-->
        </figure>

      <p>Let’s focus even more and look at how a single early curve detector connects to sophisticated curve detector in the same orientation.

      <p>In this case, our model is implementing a 5x5 convolution, so the weights linking these two neurons are a 5x5 set of weights, which can be positive or negative. A positive weight means that if the earlier neuron fires in that position, it excites the late neuron. Conversely a negative weight would mean that it inhibits it.

      <p>What we see are strong positive weights, arranged in the shape of the curve detector. We can think of this as meaning that, at each point along the curve, our curve detector is looking for a “tangent curve” using the earlier curve detector.


      <figure class="curve-figure l-body-outset">
        <div>
          <img src="./images/curve-weights-a.png"/>
          <figcaption>The raw weights between the early curve detector and late curve detector in the same orientation are curve of positive weights</figcaption>
        </div>
        <div>
          <img src="./images/curve-weights-b.png"/>
          <figcaption>This can be interpreted as looking for “tangent curves”
at each point along the curve.</figcaption>
        </div>
        <!--<figcaption></figcaption>-->
      </figure>

      <p>This is true for every pair of early and full curve detectors in similar orientations. At every point along the curve, it detects the curve in a similar orientation. Similarly, curves in the opposite orientation are inhibitory at every point along the curve.

      <figure class="curve-oreientaions-figure ">
        <div style="flex-basis: 45%;">
          <div class="figcaption" style="height: 30px; margin-bottom: 20px;">Curve detectors are <span style="color: #191;">excited</span> by earlier detectors <br> in <b>similar orientations</b>...</div>
          <img src="./images/curve-orientations-a.png" />
        </div>
        <div style="flex-basis: 5%; margin-right: 60px;">
        </div>
        <div style="flex-basis: 44.8%;">
          <div class="figcaption" style="height: 30px; margin-bottom: 20px;">... and <span style="color: #911;">inhibited</span> by earlier detectors in <br> <b>opposing orientations</b>.</div>
          <img src="./images/curve-orientations-b.png" />
        </div>
      </figure>

      <p>It’s worth reflecting here that we’re looking at neural network weights and they’re meaningful.

      <p>And the structure gets richer the closer you look. For example, if you look at an early curve detector and full curve detector in similar, but not exactly the same orientation you can often see it have stronger positive weights on the side of the curve it is more aligned with.

      <p>It’s also worth noting how the weights rotate with the orientation of the curve detector. The symmetry of the problem is reflected as a symmetry in the weights. We call circuits with exhibiting this phenomenon "equivariant circuits", and will discuss it in a later article.

      <p>[Read more about the curve detector circuit]

      <h3 id="claim-2-dog">Circuit 2: Oriented Dog Head Detection</h3>

      <p>The curve detector circuit is a low-level circuit and only spans two layers. In this section, we’ll discuss a higher-level circuit spanning across four layers. This circuit will also teach us about how neural networks implement sophisticated invariances.

      <p>Remember that a huge part of what an ImageNet model has to do is tell apart different animals. In particular, it has to distinguish between a hundred different species of dogs! And so, unsurprisingly, it develops a large number of neurons dedicated to recognizing dog related features, including heads.

      <p>Within this “dog recognition” system, one circuit strikes us as particularly interesting: a collection of neurons that handle dog heads facing to the left and dog heads facing to the right. Over three layers, the network maintains two mirrored pathways, detecting analogous units facing to the left and to the right. At each step, these pathways try to inhibit each other, sharpening the contrast. Finally, it creates invariant neurons which respond to both pathways.

      <figure style="grid-column-start: page-start; grid-column-end: text-end;">
        <img src="./images/dog-circuit-2.png"/>
        <!--<figcaption></figcaption>-->
      </figure>

      <p>We call this pattern “unioning over cases”. The network separately detects two cases (left and right) and then unions over them to create invariant units.

      <p>This circuit is striking because the network could have easily done something much less sophisticated.
        It could easily create invariant neurons by not caring very much about where the eyes, fur and snout went, and just looking for a jumble of them together.
        But instead, the network has learned to carve apart the left and right cases and handle the separately. We’re somewhat surprised that gradient descent could learn to do this!<d-footnote>To be clear, there are also more direct pathways by which various constituents of heads influence these later head detectors, without going through the left and right pathways</d-footnote>
      </p>

      <p>But this summary of the circuit is only scratching the surface of what is going on. Every connection between neurons is a convolution, so we can also look at where an input neuron excites the the next one. And the models tends to be doing what you might have optimistically hoped. For example, consider these “head with neck” units. The head is only detected on the correct side:

      <figure style="max-width: 550px;">
        <img src="./images/oriented-dog-heads.png"/>
        <!--<figcaption></figcaption>-->
      </figure>

      <p>There’s a lot more to say about this circuit, so we’ve dedicated an entire article to analyzing it in depth, including testing our theory of the circuit by editing the weights.

      [Read more about the oriented dog head circuit]

      <h3 id="claim-2-superposition">Circuit 3: Cars in Superposition</h3>

      <p>In mixed4c, a mid-late layer of InceptionV1, there is a car detecting neuron. Using features from the previous layers, it looks for wheels at the bottom of its convolutional window, and windows at the top.

      <p>But then the model does something surprising. Rather than create another pure car detector at the next layer, it spreads its car feature over a number of neurons that seem to primarily be doing something else &mdash; in particular, dog detectors.

        <figure>
          <img src="./images/superposition.png"/>
          <!--<figcaption></figcaption>-->
        </figure>

      <p>This circuit suggests that polysemantic neurons are, in some sense, deliberate. That is, you could imagine a world where the process of detecting cars and dogs were deeply intertwined in the model for some reason, and as a result polysemantic neurons were difficult to avoid. But what we’re seeing here is that the model had a “pure neuron” and then mixed it up with other features.

      <p>We call this phenomenon “superposition.”

      <p>Why would it do such a thing? We believe superposition allows the model to use fewer neurons, conserving them for more important tasks. As long as cars and dogs don’t co-occur, the model can accurately retrieve the dog feature in a later layer, allowing it to store the feature without dedicating a neuron.<d-footnote>Fundamentally, this is a property of the geometry of high-dimensional spaces, which only allow for n orthogonal vectors, but exponentially many almost orthogonal vectors. </d-footnote>

      <hr>



      <h2 id="claim-3">Claim 3: Universality</h2>

      <p class="claim-quote">Analogous features and circuits form across models and tasks.

      <p>In some ways, it may not be that surprising once you accept the earlier claims. It’s a widely accepted fact that the first layer of vision models trained on natural images will learn Gabor filters. There’s also significant research showing that, in some aggregate sense, neural networks learn representations at hidden layers that share a lot of information. Once you accept that there are meaningful features in later layers, is it surprising that the same features would also form in layers beyond the first one? And once you believe there are analogous features, isn’t it expected that they’d connect in the same ways?

      <p>Unlike the first two claims, it wouldn’t be completely fatal to circuits research if this claim turned out to be false. But it does greatly inform what kind of research makes sense. We introduced circuits as a kind of “cellular biology of deep learning.” But imagine a world where every species had cells with a completely different set of organelles and proteins. Would it still make sense to study cells in general, or would we limit ourselves to the narrow study of a few kinds of particularly important species of cells? Similarly, imagine the study of anatomy in a world where every species of animal had a completely unrelated anatomy: would we seriously study anything other than humans and a couple domestic animals?

      <p>In this same way, the universality hypothesis determines what form of circuits research makes sense. If it was true in the strongest sense, one could imagine a kind of “periodic table of visual features” which we observe and catalogue across models. On the other hand, if it was mostly false, we would need to focus on a handful of models of particular societal importance and hope they stop changing every year. There might also be in between worlds, where some lessons transfer between models but others need to be learned from scratch.

      <p>So, is the universality hypothesis true? Unfortunately, it’s the claim we have the least evidence for. To date, we simply haven’t invested enough in the comparative study of features and circuits to give strong answers.

      <p>The main piece of evidence is that we quite reliably observe the same low level features across a large variety of vision model architectures (including AlexNet, InceptionV1, InceptionV3, and residual networks) and in models trained on Places365 instead of ImageNet. We’ve also observed them repeatedly form in vanilla conv nets trained from scratch on ImageNet. For high-level features, we only have very anecdotal evidence that sometimes similar features, such as various kinds of head detectors, form repeatedly.
      </p>

      <style>
        .diagram-universality {
          display: grid;
          grid-column: text-start / page-end;
          grid-gap: 1rem;
          grid-template-columns: 1fr;
          justify-content: center;
        }

        @media (min-width: 1180px) {
          .diagram-universality {
            grid-column: page;
            grid-template-columns: 528px 448px;
          }
          .diagram-universality > figure:last-of-type .info {
            display: none;
          }
          .diagram-universality > figure:last-of-type li {
            grid-template-columns: 1fr;
          }
          .diagram-universality > figure:last-of-type > figcaption {
            padding-left: unset;
          }
        }

        .diagram-universality h3 {
          margin-top: 0;
        }

        .diagram-universality h4 {
          margin: 4px 0;
        }

        .diagram-universality > figure {
          margin: 0;
        }

        .diagram-universality > figure > figcaption {
          padding-left: 9rem;
        }

        .diagram-universality ul {
          list-style: none;
          padding-left: unset;
        }

        .diagram-universality li {
          display: grid;
          grid-template-columns: 9rem 1fr;
        }

        .diagram-universality li .images {
          display: grid;
          grid-template-columns: repeat( auto-fill, 88px);
          grid-gap: 8px;
        }

        .diagram-universality img {
          width: 88px;
          height: 88px;
          background-color: gray;
          border-radius: 4px;
          object-fit: none;
        }
      </style>
      <figure class="diagram-universality" role="group">
        <figure>
          <figcaption>
            <h3>Curve detectors</h3>
          </figcaption>
          <ul>
            <li>
              <div class="info">
                <h4>AlexNet</h4>
                <span class="figcaption">Krizhevsky et&#8239;al.<d-cite key="krizhevsky2012alexnet"></d-cite></span>
              </div>
              <div class="images">
                <!-- [257, 348, 253, 282, 277, 319] -->
                <img src="images/universality/curves/AlexNet/unit-0.png"/>
                <img src="images/universality/curves/AlexNet/unit-1.png"/>
                <img src="images/universality/curves/AlexNet/unit-2.png"/>
                <img src="images/universality/curves/AlexNet/unit-3.png"/>
              </div>
            </li>
            <li>
              <div class="info">
                <h4>GoogLeNet</h4>
                <span class="figcaption">Szegedy et&#8239;al.<d-cite key="szegedy2015googlenet"></d-cite></span>
              </div>
              <div class="images">
                <!-- [379, 406, 385, 343, 342, 388, 340, 330, 349, 324] -->
                <img src="images/universality/curves/GoogLeNet/unit-0.png"/>
                <img src="images/universality/curves/GoogLeNet/unit-1.png"/>
                <img src="images/universality/curves/GoogLeNet/unit-2.png"/>
                <img src="images/universality/curves/GoogLeNet/unit-3.png"/>
              </div>

            </li>
            <li>
              <div class="info">
                <h4>VGG19</h4>
                <span class="figcaption">Simonyan et&#8239;al.<d-cite key="simonyan2014vggnet"></d-cite></span>
              </div>
              <div class="images">
                <!-- [82, 79, 102, 110, 90, 104] -->
                <img src="images/universality/curves/VGG19/unit-0.png"/>
                <img src="images/universality/curves/VGG19/unit-1.png"/>
                <img src="images/universality/curves/VGG19/unit-2.png"/>
                <img src="images/universality/curves/VGG19/unit-3.png"/>
              </div>
            </li>
            <li>
              <div class="info">
                <h4>ResNetV2-50</h4>
                <span class="figcaption">Kaiming et&#8239;al.<d-cite key="kaiming2015resnet"></d-cite></span>
              </div>
              <div class="images">
                <img src="images/universality/curves/ResNetV2/unit-0.png"/>
                <img src="images/universality/curves/ResNetV2/unit-1.png"/>
                <img src="images/universality/curves/ResNetV2/unit-2.png"/>
                <img src="images/universality/curves/ResNetV2/unit-3.png"/>
              </div>
            </li>
          </ul>
        </figure>
        <figure>
          <figcaption>
            <h3>High-Low Frequency detectors</h3>
          </figcaption>
          <ul>
            <li>
              <div class="info">
                <h4>AlexNet</h4>
                <span class="figcaption">Krizhevsky et&#8239;al.<d-cite key="krizhevsky2012alexnet"></d-cite></span>
              </div>
              <div class="images">
                <img src="images/universality/hilo/AlexNet/unit-0.png"/>
                <img src="images/universality/hilo/AlexNet/unit-1.png"/>
                <img src="images/universality/hilo/AlexNet/unit-2.png"/>
                <img src="images/universality/hilo/AlexNet/unit-3.png"/>
              </div>
            </li>
            <li>
              <div class="info">
                <h4>GoogLeNet</h4>
                <span class="figcaption">Szegedy et&#8239;al.<d-cite key="szegedy2015googlenet"></d-cite></span>
              </div>
              <div class="images">
                <img src="images/universality/hilo/GoogLeNet/unit-0.png"/>
                <img src="images/universality/hilo/GoogLeNet/unit-1.png"/>
                <img src="images/universality/hilo/GoogLeNet/unit-2.png"/>
                <img src="images/universality/hilo/GoogLeNet/unit-3.png"/>
              </div>

            </li>
            <li>
              <div class="info">
                <h4>VGG19</h4>
                <span class="figcaption">Simonyan et&#8239;al.<d-cite key="simonyan2014vggnet"></d-cite></span>
              </div>
              <div class="images">
                <img src="images/universality/hilo/VGG19/unit-0.png"/>
                <img src="images/universality/hilo/VGG19/unit-1.png"/>
                <img src="images/universality/hilo/VGG19/unit-2.png"/>
                <img src="images/universality/hilo/VGG19/unit-3.png"/>
              </div>
            </li>
            <li>
              <div class="info">
                <h4>ResNetV2-50</h4>
                <span class="figcaption">Kaiming et&#8239;al.<d-cite key="kaiming2015resnet"></d-cite></span>
              </div>
              <div class="images">
                <img src="images/universality/hilo/ResNetV2/unit-0.png"/>
                <img src="images/universality/hilo/ResNetV2/unit-1.png"/>
                <img src="images/universality/hilo/ResNetV2/unit-2.png"/>
                <img src="images/universality/hilo/ResNetV2/unit-3.png"/>
              </div>
            </li>
          </ul>
        </figure>
      </figure>

      <p>
        These results have led us to believe that a weak version of the universality hypothesis is likely true, but further work will be needed to understand if the apparent universality of some low-level vision features is the exception or the rule.
        If it turns out that the universality hypothesis is broadly true in neural networks,
        it will be really tempting to speculate: might biological neural networks also learn similar features?

      <hr>

      <h2 id="natural-science">Interpretability as a Natural Science</h2>

      <p>
        <i>The Structure of Scientific Revolutions</i> by Thomas Kuhn<d-cite bibtex-key="kuhn1962structure"></d-cite> is a classic text on the history and sociology of science.
        In it, Kuhn distinguishes between “normal science” in which a scientific community has a paradigm, and “extraordinary science” in which a community lacks a paradigm, either because it never had one or because it was weakened by crisis.
        It's worth noting that "extraordinary science" is not a desirable state: it's a period where researchers struggle to be productive.

      <p>
        Kuhn's description of pre-paradigmatic fields feel eerily reminiscent of interpretability today.<d-footnote>
          We were introduced to Kuhn's work and this connection by conversations with Tom McGrath at DeepMind</d-footnote>
        There isn’t consensus on what the objects of study are, what methods we should use to answer them, or how to evaluate research results.
        To quote a recent interview with Ian Goodfellow: "For interpretability, I don't think we even have the right definitions."<d-cite bibtex-key="goodfellow2019interview"></d-cite>
      </P>

      <p>
        One particularly challenging aspect of being in a pre-paradigmatic field is that there isn't a shared sense of how to evaluate work in interpretability.
        There are two common proposals for dealing with this, drawing on the standards of adjacent fields. Some researchers, especially those with a deep learning background, want an “interpretability benchmark” which can evaluate how effective an interpretability method is. Other researchers with an HCI background may wish to evaluate interpretability methods through user studies.

      <p>
        But interpretability could also borrow from a third paradigm: natural science.
        In this view, neural networks are an object of empirical investigation, perhaps similar to an organism in biology. Such work would try to make empirical claims about a given network, which could be held to the standard of falsifiability.

      <p>
        Why don’t we see more of this kind of evaluation of work in interpretability and visualization?<d-footnote>
          To be clear, we do see researchers who take more of this natural science approach, especially in earlier interpretability research. It just seems less common right now.</d-footnote>
        Especially given that there’s so much adjacent ML work which does adopt this frame!
        One reason might be that it’s very difficult to make robustly true statements about the behavior of a neural network as a whole.
        They’re incredibly complicated objects.
        It’s also hard to formalize what the interesting empirical statements about them would, exactly, be.
        And so we often get standards of evaluations more targeted at whether an interpretability method is useful rather than whether we’re learning true statements.

      <p>
        Circuits side steps these challenges by focusing on tiny subgraphs of a neural network for which rigorous empirical investigation is tractable.
        They’re very much falsifiable: for example, if you understand a circuit, you should be able to predict what will change if you edit the weights.
        In fact, for small enough circuits, statements about their behavior become questions of mathematical reasoning.
        Of course, the cost of this rigor is that statements about circuits are much smaller in scope than overall model behavior.
        But it seems like, with sufficient effort, statements about model behavior could be broken down into statements about circuits.
        If so, perhaps circuits could act as a kind of epsitemtic foundation for interpretability.
      </p>


      <hr>

      <h2 id="closing">Closing Thoughts</h2>

      <p>We take it for granted that the microscope is an important scientific instrument. It’s practically a symbol of science. But this wasn’t always the case, and microscopes didn’t initially take off as a scientific tool. In fact, they seem to have languished for around fifty years. The turning point was when Robert Hooke published Micrographia, a collection of drawings of things he’d seen using a microscope, including the first picture of a cell.

      <p>
        My impression is that there is some anxiety in the interpretability community that we aren’t taken very seriously.
        That this research is too qualitative.
        That it isn't very scientific.
        But the lesson of the microscope and cellular biology is that perhaps this is expected.
      </p>

      <p>The discovery of cells was, after all, a qualitative research result. And it changed the world.</p>

      <br><br><br><br>



      <section id="thread-nav" class="thread-info">
          <div class="icon-multiple-pages"></div>
          <p class="explanation">
              This article is part of the Circuits thread, a collection of short articles and commentary delving into the inner workings of neural networks.
              Circuits aims to update weekly on Tuesdays.
              <a class="overview" href="#">Thread Overview</a>
          </p>

          <a class="prev" href="#">An Overview of early vision</a>
          <a class="next" href="#">Circuit Detectors</a>
      </section>





    </d-article>

    <d-appendix>
      <h3 id="glossary">Glossary</h3>

      <p><b>Circuit</b> - A subgraph of a neural network. Nodes correspond to neurons or linear combinations of neurons.
        Two nodes have an edge between them if they are in adjacent layers. The edges have weights which are the weights between those neurons (or <i>n<sub>1</sub>Wn<sub>2</sub><sup style="margin-left:-4px;">T</sup></i> if the nodes are linear combinations). For convolutional layers, the weights are 2D matrices representing the weights for  different relative positions of the layers.</p>
      <p><b>Client Neuron</b> - A neuron in a later layer which relies on a particular earlier neuron. For example, a circle detector is a client of curve detectors.</p>
      <p><b>Direction</b> - A linear combination of neurons in a layer. Equivalently, a vector in the representation of a layer. A direction can be an individual neuron (which is a basis direction of the vector space). For intuition about directions as an object, see <a href="https://distill.pub/2018/building-blocks/">Building Blocks</a><d-cite bibtex-key="olah2018building"></d-cite> (in particular, the section titled "What Does the Network See?") and <a href="https://distill.pub/2019/activation-atlas/">Activation Atlases</a><d-cite bibtex-key="carter2019activation"></d-cite>.</p>
      <p><b>Downstream / Upstream</b> - In a later layer / In an earlier layer.</p>
      <p><b>Feature</b> - A scalar function of the input. In this essay, neural network features are <b>directions</b>, and often simply individual neurons. We claim such features in neural networks are typically <b>meaningful features</b> which can be rigorously studied (<a href="claim-1">Claim 1</a>).</p>
      <p><b>Meaningful Feature</b> - A feature that genuinely responds to an articulable property of the input, such as the presence of a curve or a floppy ear. Meaningful features may still be noisy or imperfect.</p>
      <p><b>Representation</b> - The vector space formed by the activations of all neurons in a layer, with vectors of the form <i>(activation of neuron 1, activation of neuron 2, ...)</i>. A representation can be thought of as the collection of all features that exist in a layer. For intuition about representations in vision models, see <a href="https://distill.pub/2019/activation-atlas/">Activation Atlases</a><d-cite bibtex-key="carter2019activation"></d-cite>.</p>


      <h3>Historical Note</h3>
      <p>
        The content of this essay was previously presented as a keynote talk by Chris Olah at <a href="https://visxai.io/">VISxAI</a> 2019.
        It was also informally presented at MILA, the Vector Institute, the Redwood Center for Neuroscience, and a private workshop.
      </p>

      <h3>Acknowledgments</h3>
      <p>
        All our work understanding InceptionV1 is indebted to Alex Mordvintsev, who's early explorations of vision models paved pathways we still follow.
        We're deeply grateful to Nick Barry, and Sophia Sanborn for their deep engagement on potential connections between our work and neuroscience,
        and to Tom McGrath who pointed out the similarities between Kuhn's "pre-paradigmatic science" and the state interpretability as a field to us.
        The careful comments and criticism of Brice Menard were also invaluable in sharpening this essay.
      </p>
      <p>
        In addition to Nick and Sophia's deep engagement, we're more generally appreciative of the neuroscience community's engagement with us,
        especially in sharing hard-won lessons about methodological weaknesses in our work.
        In particular, we appreciate Brian Wandell pushing us in 2018 on not using tuning curves and the importance of families of neurons, which we think has made our  work much stronger.
        We're also very grateful for the comments of Mareike Grotheer, Natalia Bilenko, Bruno Olshausen, and Jascha Sohl-Dickstein.
        [MORE]
        We think we have a lot to learn from the neuroscience community and are excited to continue doing so.
      </p>
      <p>
        One of the  biggest privileges of working on circuits has been the open collaboration and feedback in the <a href="https://slack.distill.pub">Distill slack</a>'s #circuits channel.
        We've especially appreciated the detailed feedback we received from Stefan Sietzen, Shahab Bakhtiari, and Flora Liu.
        [MORE]
        (Stefan has additionally run with many of these ideas, and we're excited to see his work in future articles in this thread!)
      </p>
      <p>
        Finally, we're grateful for the institutional support of OpenAI, and for the support and comments of all our colleagues and friends across institutions, including Dario Amodei, Daniela Amodei, Taco Cohen, Katarina Slama, Alethea Power, Jacob Hilton, Jacob Steinhardt, Tom Brown, Preetum Nakkiran, Ilya Sutskever, Ryan Lowe, Erin McCloskey, and Arvind Satyanarayan.
        [MORE]
      </p>


      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>

    <script src="https://distill.pub/template.v2.js"></script>
    <link rel="stylesheet" type="text/css" href="index.css">

    <script src="./index.js"></script>
  </body>
</html>
